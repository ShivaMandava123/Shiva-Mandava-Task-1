{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeKlWLMXSIk8GhvbXhtju+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivaMandava123/Shiva-Mandava-Task-1/blob/main/twitter%20scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXpjVvDg4lK5"
      },
      "outputs": [],
      "source": [
        "import snscrape\n",
        "import pandas as pd\n",
        "from pymongo import MongoClient\n",
        "import streamlit as st\n",
        "\n",
        "# Scrape data from Twitter using snscrape library\n",
        "def scrape_data(keyword, date_range, tweet_limit):\n",
        "    tweets = snscrape.modules.Twitter.TwitterSearchScraper(keyword, limit=tweet_limit).get_items()\n",
        "    data = []\n",
        "    for tweet in tweets:\n",
        "        data.append({\n",
        "            'date': tweet.time,\n",
        "            'id': tweet.id,\n",
        "            'url': tweet.url,\n",
        "            'content': tweet.content,\n",
        "            'user': tweet.user.name,\n",
        "            'reply_count': tweet.replies,\n",
        "            'retweet_count': tweet.retweets,\n",
        "            'language': tweet.language,\n",
        "            'source': tweet.source,\n",
        "            'like_count': tweet.likes\n",
        "        })\n",
        "    # Create a dataframe with the scraped data\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Store data in MongoDB\n",
        "def store_data(df, keyword):\n",
        "    client = MongoClient()\n",
        "    db = client['twitter_scraped_data']\n",
        "    collection_name = keyword + \"_\" + str(pd.Timestamp.now())\n",
        "    db[collection_name].insert_many(df.to_dict('records'))\n",
        "\n",
        "# Create a GUI using Streamlit\n",
        "def main():\n",
        "    st.title(\"Twitter Data Scraper\")\n",
        "    keyword = st.text_input(\"Enter keyword or hashtag to search for:\")\n",
        "    date_range = st.date_range_input(\"Select date range:\")\n",
        "    tweet_limit = st.number_input(\"Enter limit for number of tweets to scrape:\")\n",
        "    if st.button(\"Scrape\"):\n",
        "        df = scrape_data(keyword, date_range, tweet_limit)\n",
        "        st.dataframe(df)\n",
        "        if st.button(\"Store in MongoDB\"):\n",
        "            store_data(df, keyword)\n",
        "            st.success(\"Data stored in MongoDB\")\n",
        "        if st.button(\"Download as CSV\"):\n",
        "            csv = df.to_csv()\n",
        "            b64 = base64.b64encode(csv.encode()).decode()  # some strings <-> bytes conversions necessary here\n",
        "            href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{keyword}_tweets.csv\">Download CSV File</a>'\n",
        "            st.markdown(href, unsafe_allow_html=True)\n",
        "        if st.button(\"Download as JSON\"):\n",
        "            json = df.to_json()\n",
        "            b64 = base64.b64encode(json.encode()).decode()  # some strings <-> bytes conversions necessary here\n",
        "            href = f'<a href=\"data:file/json;base64,{b64}\" download=\"{keyword}_tweets.json\">Download JSON File</a>'\n",
        "            st.markdown(href, unsafe_allow_html=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}